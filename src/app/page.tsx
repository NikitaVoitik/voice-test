"use client";

import { useEffect, useState, useRef, useCallback } from "react";
import { LiveClient, LiveTranscriptionEvents, createClient } from "@deepgram/sdk";

// ElevenLabs voice options
const ELEVENLABS_VOICES = [
  { id: "21m00Tcm4TlvDq8ikWAM", name: "Rachel" },
  { id: "AZnzlk1XvdvUeBnXmlld", name: "Domi" },
  { id: "EXAVITQu4vr4xnSDxMaL", name: "Bella" },
  { id: "ErXwobaYiN019PkySvjV", name: "Antoni" },
  { id: "MF3mGyEYCl7XYWbV9V6O", name: "Elli" },
  { id: "TxGEqnHWrfWFTfGW9XjX", name: "Josh" },
  { id: "VR6AewLTigWG4xSOukaG", name: "Arnold" },
  { id: "pNInz6obpgDQGcFmaJgB", name: "Adam" },
  { id: "yoZ06aMxZJJ28mfd3POQ", name: "Sam" },
];

const ELEVENLABS_MODELS = [
  { id: "eleven_monolingual_v1", name: "Eleven Monolingual v1" },
  { id: "eleven_multilingual_v1", name: "Eleven Multilingual v1" },
  { id: "eleven_multilingual_v2", name: "Eleven Multilingual v2" },
  { id: "eleven_turbo_v2", name: "Eleven Turbo v2" },
  { id: "eleven_turbo_v2_5", name: "Eleven Turbo v2.5" },
];

// Cartesia voice options
const CARTESIA_VOICES = [
  { id: "a0e99841-438c-4a64-b679-ae501e7d6091", name: "Barbershop Man" },
  { id: "79a125e8-cd45-4c13-8a67-188112f4dd22", name: "British Lady" },
  { id: "95856005-0332-41b0-935f-352e296aa0df", name: "Calm Lady" },
  { id: "69267136-1bdc-412f-ad78-0caad210fb40", name: "Classy British Man" },
  { id: "726d5ae5-055f-4c3d-8355-d9677de68937", name: "Conversational Lady" },
  { id: "fb26447f-308b-471e-8b00-8e9f04284eb5", name: "Friendly Reading Man" },
  { id: "41534e16-2966-4c6b-9670-111411def906", name: "Newsman" },
  { id: "156fb8d2-335b-4950-9cb3-a2d33befec77", name: "Wise Lady" },
];

const CARTESIA_MODELS = [
  { id: "sonic-english", name: "Sonic English" },
  { id: "sonic-multilingual", name: "Sonic Multilingual" },
];

export default function Home() {
  const [elevenLabsLoading, setElevenLabsLoading] = useState(false);
  const [cartesiaLoading, setCartesiaLoading] = useState(false);
  const [elevenLabsAudio, setElevenLabsAudio] = useState<string | null>(null);
  const [cartesiaAudio, setCartesiaAudio] = useState<string | null>(null);
  const [error, setError] = useState<string | null>(null);
  
  // ElevenLabs selection state
  const [elevenLabsVoiceId, setElevenLabsVoiceId] = useState(ELEVENLABS_VOICES[0].id);
  const [elevenLabsModelId, setElevenLabsModelId] = useState(ELEVENLABS_MODELS[0].id);
  
  // Cartesia selection state
  const [cartesiaVoiceId, setCartesiaVoiceId] = useState(CARTESIA_VOICES[0].id);
  const [cartesiaModelId, setCartesiaModelId] = useState(CARTESIA_MODELS[0].id);
  
  // Deepgram realtime transcription state
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [isConnecting, setIsConnecting] = useState(false);
  const deepgramConnectionRef = useRef<LiveClient | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  
  // Audio streaming state
  const elevenLabsAudioContextRef = useRef<AudioContext | null>(null);
  const cartesiaAudioContextRef = useRef<AudioContext | null>(null);
  const elevenLabsSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const cartesiaSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const [isGeneratingAudio, setIsGeneratingAudio] = useState(false);
  const keepaliveIntervalRef = useRef<NodeJS.Timeout | null>(null);
  
  // OpenAI streaming state
  const [openAIResponse, setOpenAIResponse] = useState("");
  const [isStreamingOpenAI, setIsStreamingOpenAI] = useState(false);
  const openAIAbortControllerRef = useRef<AbortController | null>(null);
  
  // Timing measurements
  const [elevenLabsAudioTime, setElevenLabsAudioTime] = useState<number | null>(null);
  const [cartesiaAudioTime, setCartesiaAudioTime] = useState<number | null>(null);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (elevenLabsAudio) {
        URL.revokeObjectURL(elevenLabsAudio);
      }
      if (cartesiaAudio) {
        URL.revokeObjectURL(cartesiaAudio);
      }
      if (keepaliveIntervalRef.current) {
        clearInterval(keepaliveIntervalRef.current);
      }
      if (deepgramConnectionRef.current) {
        deepgramConnectionRef.current.finish();
      }
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, [elevenLabsAudio, cartesiaAudio]);

  const toggleRecording = useCallback(async () => {
    if (isRecording) {
      // Stop recording
      setIsRecording(false);
      
      if (keepaliveIntervalRef.current) {
        clearInterval(keepaliveIntervalRef.current);
        keepaliveIntervalRef.current = null;
      }
      
      if (deepgramConnectionRef.current) {
        deepgramConnectionRef.current.finish();
        deepgramConnectionRef.current = null;
      }
      
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
        mediaStreamRef.current = null;
      }
    } else {
      // Start recording
      setError(null);
      setTranscript("");
      setIsConnecting(true);
      setElevenLabsAudioTime(null);
      setCartesiaAudioTime(null);
      
      try {
        // Get microphone stream first
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaStreamRef.current = stream;
        console.log("Microphone stream obtained:", stream.getAudioTracks().length, "audio tracks");
        
        // Create MediaRecorder BEFORE establishing Deepgram connection
        const mediaRecorder = new MediaRecorder(stream, {
          mimeType: "audio/webm",
        });
        mediaRecorderRef.current = mediaRecorder;
        console.log("MediaRecorder created");
        
        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0 && deepgramConnectionRef.current) {
            console.log("Sending audio chunk to Deepgram:", event.data.size, "bytes");
            deepgramConnectionRef.current.send(event.data);
          }
        };
        
        mediaRecorder.onerror = (event) => {
          console.error("MediaRecorder error:", event);
        };
        
        // Get API key from backend
        const keyResponse = await fetch("/api/deepgram");
        if (!keyResponse.ok) {
          throw new Error("Failed to get Deepgram API key");
        }
        const { key } = await keyResponse.json();
        console.log("Deepgram API key obtained");
        
        // Create Deepgram client and connection
        const deepgram = createClient(key);
        const connection = deepgram.listen.live({
          model: "nova-2",
          language: "en-US",
          smart_format: true,
          interim_results: true,
          keepalive: true,
        });
        
        deepgramConnectionRef.current = connection;
        
        // Set up event listeners
        connection.on(LiveTranscriptionEvents.Open, () => {
          console.log("Deepgram WebSocket connection opened");
          setIsConnecting(false);
          setIsRecording(true);
          
          // Start sending audio immediately when connection opens
          if (mediaRecorderRef.current && mediaRecorderRef.current.state === "inactive") {
            console.log("Starting MediaRecorder...");
            mediaRecorderRef.current.start(250);
            console.log("MediaRecorder started, state:", mediaRecorderRef.current.state);
          } else {
            console.warn("MediaRecorder not ready or already started:", mediaRecorderRef.current?.state);
          }
          
          // Set up keepalive to prevent connection timeout
          keepaliveIntervalRef.current = setInterval(() => {
            if (deepgramConnectionRef.current) {
              try {
                // Send keepalive message
                deepgramConnectionRef.current.keepAlive();
                console.log("Keepalive sent to Deepgram");
              } catch (e) {
                console.error("Failed to send keepalive:", e);
              }
            }
          }, 5000); // Send keepalive every 5 seconds
        });
        
        connection.on(LiveTranscriptionEvents.Transcript, (data) => {
          const transcript = data.channel.alternatives[0].transcript;
          if (transcript && transcript.trim().length > 0) {
            console.log("Transcript received:", transcript, "is_final:", data.is_final);
            setTranscript((prev) => {
              const newTranscript = data.is_final 
                ? prev + (prev ? " " : "") + transcript
                : prev;
              
              // Don't auto-generate audio from transcript anymore
              // User will manually submit the prompt
              
              return newTranscript;
            });
          }
        });
        
        connection.on(LiveTranscriptionEvents.Metadata, (data) => {
          console.log("Deepgram metadata:", JSON.stringify(data, null, 2));
        });
        
        connection.on(LiveTranscriptionEvents.Error, (error) => {
          console.error("Deepgram error:", error);
          setError("Transcription error occurred");
          setIsRecording(false);
          setIsConnecting(false);
          if (keepaliveIntervalRef.current) {
            clearInterval(keepaliveIntervalRef.current);
            keepaliveIntervalRef.current = null;
          }
        });
        
        connection.on(LiveTranscriptionEvents.Close, (event) => {
          console.log("Deepgram connection closed:", event);
          setIsRecording(false);
          setIsConnecting(false);
          if (keepaliveIntervalRef.current) {
            clearInterval(keepaliveIntervalRef.current);
            keepaliveIntervalRef.current = null;
          }
        });
      } catch (err) {
        console.error("Failed to start recording:", err);
        setError("Failed to access microphone or connect to Deepgram. Please check your setup.");
        setIsRecording(false);
        setIsConnecting(false);
      }
    }
  }, [isRecording]);
  
  const handleSubmitPrompt = async () => {
    if (!transcript.trim() || isStreamingOpenAI) return;
    
    setOpenAIResponse("");
    setIsStreamingOpenAI(true);
    setElevenLabsLoading(true);
    setCartesiaLoading(true);
    setElevenLabsAudio(null);
    setCartesiaAudio(null);
    
    const abortController = new AbortController();
    openAIAbortControllerRef.current = abortController;
    
    try {
      // Start streaming from OpenAI
      const response = await fetch("/api/openai", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ prompt: transcript }),
        signal: abortController.signal,
      });
      
      if (!response.ok) {
        throw new Error("Failed to get OpenAI response");
      }
      
      const reader = response.body?.getReader();
      if (!reader) throw new Error("No response body");
      
      const decoder = new TextDecoder();
      let accumulatedText = "";
      let buffer = "";
      
      // Start audio generation for both providers
      const elevenLabsChunks: string[] = [];
      const cartesiaChunks: string[] = [];
      
      const elevenLabsStartTime = performance.now();
      const cartesiaStartTime = performance.now();
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        
        for (const line of lines) {
          if (line.trim()) {
            try {
              const { content } = JSON.parse(line);
              accumulatedText += content;
              setOpenAIResponse(accumulatedText);
              
              // Accumulate chunks for voice generation
              elevenLabsChunks.push(content);
              cartesiaChunks.push(content);
            } catch (e) {
              console.error("Failed to parse chunk:", e);
            }
          }
        }
      }
      
      // Generate audio from the complete response
      if (accumulatedText.trim()) {
        const elevenLabsPromise = streamAudioFromProvider(
          accumulatedText,
          "elevenlabs",
          elevenLabsVoiceId,
          elevenLabsModelId,
          elevenLabsStartTime
        );
        
        const cartesiaPromise = streamAudioFromProvider(
          accumulatedText,
          "cartesia",
          cartesiaVoiceId,
          cartesiaModelId,
          cartesiaStartTime
        );
        
        await Promise.all([elevenLabsPromise, cartesiaPromise]);
      }
    } catch (err: any) {
      if (err.name !== "AbortError") {
        console.error("OpenAI streaming error:", err);
        setError("Failed to get response from OpenAI");
      }
    } finally {
      setIsStreamingOpenAI(false);
      openAIAbortControllerRef.current = null;
    }
  };
  
  const streamAudioFromProvider = async (
    text: string,
    provider: "elevenlabs" | "cartesia",
    voiceId: string,
    modelId: string,
    startTime: number
  ) => {
    const isElevenLabs = provider === "elevenlabs";
    const setLoading = isElevenLabs ? setElevenLabsLoading : setCartesiaLoading;
    const setAudio = isElevenLabs ? setElevenLabsAudio : setCartesiaAudio;
    const currentAudio = isElevenLabs ? elevenLabsAudio : cartesiaAudio;
    const setAudioTime = isElevenLabs ? setElevenLabsAudioTime : setCartesiaAudioTime;
    
    try {
      const requestBody = { text, voiceId, modelId };
      
      const response = await fetch(`/api/${provider}`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });
      
      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error || "Failed to generate audio");
      }
      
      // Stream the audio response
      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error("No response body");
      }
      
      const chunks: Uint8Array[] = [];
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        if (value) {
          chunks.push(value);
        }
      }
      
      // Combine all chunks into a single blob
      const blob = new Blob(chunks, { type: "audio/mpeg" });
      const audioUrl = URL.createObjectURL(blob);
      
      const audioTime = performance.now() - startTime;
      setAudioTime(audioTime);
      
      if (currentAudio) {
        URL.revokeObjectURL(currentAudio);
      }
      
      setAudio(audioUrl);
    } catch (err) {
      console.error(`${provider} audio generation error:`, err);
    } finally {
      setLoading(false);
    }
  };
  
  const generateAudioFromTranscript = async (text: string, provider: "elevenlabs" | "cartesia") => {
    const isElevenLabs = provider === "elevenlabs";
    const voiceId = isElevenLabs ? elevenLabsVoiceId : cartesiaVoiceId;
    const modelId = isElevenLabs ? elevenLabsModelId : cartesiaModelId;
    const startTime = performance.now();
    
    if (isElevenLabs) {
      setElevenLabsLoading(true);
    } else {
      setCartesiaLoading(true);
    }
    
    await streamAudioFromProvider(text, provider, voiceId, modelId, startTime);
  };
  

  return (
    <div className="min-h-screen bg-gradient-to-br from-blue-50 to-indigo-100 dark:from-gray-900 dark:to-gray-800 py-12 px-4">
      <main className="max-w-4xl mx-auto">
        <div className="text-center mb-8">
          <h1 className="text-4xl font-bold text-gray-900 dark:text-white mb-2">
            Voice Test App
          </h1>
          <p className="text-lg text-gray-600 dark:text-gray-300">
            Compare ElevenLabs vs Cartesia Text-to-Speech
          </p>
        </div>

        <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6 mb-6">
          <div className="flex justify-between items-center mb-4">
            <label
              htmlFor="transcript"
              className="block text-sm font-medium text-gray-700 dark:text-gray-300"
            >
              Your Prompt (type or dictate):
            </label>
            <button
              type="button"
              onClick={toggleRecording}
              disabled={isConnecting || isStreamingOpenAI}
              className={`px-4 py-2 rounded-lg font-medium transition-colors ${
                isRecording
                  ? "bg-red-600 hover:bg-red-700 text-white"
                  : "bg-green-600 hover:bg-green-700 text-white disabled:bg-gray-400"
              }`}
            >
              {isRecording ? "üî¥ Stop Dictation" : "üé§ Dictate"}
            </button>
          </div>
          {isConnecting && (
            <div className="mb-2 text-sm text-blue-600 dark:text-blue-400 font-medium">
              ‚è≥ Connecting to Deepgram...
            </div>
          )}
          {isRecording && (
            <div className="mb-2 text-sm text-red-600 dark:text-red-400 font-medium animate-pulse">
              üî¥ Recording... Speak now!
            </div>
          )}
          <textarea
            id="transcript"
            value={transcript}
            onChange={(e) => setTranscript(e.target.value)}
            placeholder="Type your prompt or use dictation..."
            className="w-full h-32 px-3 py-2 text-sm border border-gray-300 dark:border-gray-600 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-transparent dark:bg-gray-700 dark:text-white resize-none"
            disabled={isRecording || isStreamingOpenAI}
          />
          <button
            type="button"
            onClick={handleSubmitPrompt}
            disabled={!transcript.trim() || isStreamingOpenAI}
            className="mt-3 w-full bg-blue-600 hover:bg-blue-700 disabled:bg-gray-400 text-white font-medium py-3 px-4 rounded-lg transition-colors"
          >
            {isStreamingOpenAI ? "‚è≥ Getting Response..." : "üöÄ Submit Prompt"}
          </button>
        </div>

        {error && (
          <div className="bg-red-100 dark:bg-red-900 border border-red-400 dark:border-red-700 text-red-700 dark:text-red-200 px-4 py-3 rounded-lg mb-6">
            {error}
          </div>
        )}
        
        {openAIResponse && (
          <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6 mb-6">
            <h2 className="text-xl font-semibold text-gray-900 dark:text-white mb-3">
              ü§ñ OpenAI Response:
            </h2>
            <div className="prose dark:prose-invert max-w-none">
              <p className="text-gray-700 dark:text-gray-300 whitespace-pre-wrap">{openAIResponse}</p>
            </div>
            {isStreamingOpenAI && (
              <div className="mt-3 text-sm text-blue-600 dark:text-blue-400 animate-pulse">
                ‚è≥ Streaming response...
              </div>
            )}
          </div>
        )}

        <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
          {/* ElevenLabs Card */}
          <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6">
            <h2 className="text-2xl font-semibold text-gray-900 dark:text-white mb-4">
              ElevenLabs
            </h2>
            
            <div className="mb-4">
              <label
                htmlFor="elevenlabs-voice"
                className="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2"
              >
                Voice:
              </label>
              <select
                id="elevenlabs-voice"
                value={elevenLabsVoiceId}
                onChange={(e) => setElevenLabsVoiceId(e.target.value)}
                className="w-full px-3 py-2 border border-gray-300 dark:border-gray-600 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-transparent dark:bg-gray-700 dark:text-white"
              >
                {ELEVENLABS_VOICES.map((voice) => (
                  <option key={voice.id} value={voice.id}>
                    {voice.name}
                  </option>
                ))}
              </select>
            </div>
            
            <div className="mb-4">
              <label
                htmlFor="elevenlabs-model"
                className="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2"
              >
                Model:
              </label>
              <select
                id="elevenlabs-model"
                value={elevenLabsModelId}
                onChange={(e) => setElevenLabsModelId(e.target.value)}
                className="w-full px-3 py-2 border border-gray-300 dark:border-gray-600 rounded-lg focus:ring-2 focus:ring-indigo-500 focus:border-transparent dark:bg-gray-700 dark:text-white"
              >
                {ELEVENLABS_MODELS.map((model) => (
                  <option key={model.id} value={model.id}>
                    {model.name}
                  </option>
                ))}
              </select>
            </div>
            
            {elevenLabsLoading && (
              <div className="mb-4 p-3 bg-indigo-50 dark:bg-indigo-900/20 rounded-lg text-sm text-center">
                <div className="text-indigo-600 dark:text-indigo-400 animate-pulse">
                  üéµ Generating audio...
                </div>
              </div>
            )}
            
            {elevenLabsAudioTime !== null && (
              <div className="mb-4 p-3 bg-indigo-50 dark:bg-indigo-900/20 rounded-lg text-sm">
                <div className="font-semibold text-indigo-900 dark:text-indigo-100 mb-1">‚è±Ô∏è Timing:</div>
                <div className="text-indigo-700 dark:text-indigo-300">
                  Audio Generation: <span className="font-mono font-bold">{(elevenLabsAudioTime / 1000).toFixed(2)}s</span>
                </div>
              </div>
            )}
            
            {elevenLabsAudio && (
              <div className="mt-4">
                {/* biome-ignore lint/a11y/useMediaCaption: The audio is generated from the text input which serves as the caption */}
                <audio controls className="w-full" key={elevenLabsAudio}>
                  <source src={elevenLabsAudio} type="audio/mpeg" />
                  Your browser does not support the audio element.
                </audio>
              </div>
            )}
          </div>

          {/* Cartesia Card */}
          <div className="bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6">
            <h2 className="text-2xl font-semibold text-gray-900 dark:text-white mb-4">
              Cartesia
            </h2>
            
            <div className="mb-4">
              <label
                htmlFor="cartesia-voice"
                className="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2"
              >
                Voice:
              </label>
              <select
                id="cartesia-voice"
                value={cartesiaVoiceId}
                onChange={(e) => setCartesiaVoiceId(e.target.value)}
                className="w-full px-3 py-2 border border-gray-300 dark:border-gray-600 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent dark:bg-gray-700 dark:text-white"
              >
                {CARTESIA_VOICES.map((voice) => (
                  <option key={voice.id} value={voice.id}>
                    {voice.name}
                  </option>
                ))}
              </select>
            </div>
            
            <div className="mb-4">
              <label
                htmlFor="cartesia-model"
                className="block text-sm font-medium text-gray-700 dark:text-gray-300 mb-2"
              >
                Model:
              </label>
              <select
                id="cartesia-model"
                value={cartesiaModelId}
                onChange={(e) => setCartesiaModelId(e.target.value)}
                className="w-full px-3 py-2 border border-gray-300 dark:border-gray-600 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent dark:bg-gray-700 dark:text-white"
              >
                {CARTESIA_MODELS.map((model) => (
                  <option key={model.id} value={model.id}>
                    {model.name}
                  </option>
                ))}
              </select>
            </div>
            
            {cartesiaLoading && (
              <div className="mb-4 p-3 bg-purple-50 dark:bg-purple-900/20 rounded-lg text-sm text-center">
                <div className="text-purple-600 dark:text-purple-400 animate-pulse">
                  üéµ Generating audio...
                </div>
              </div>
            )}
            
            {cartesiaAudioTime !== null && (
              <div className="mb-4 p-3 bg-purple-50 dark:bg-purple-900/20 rounded-lg text-sm">
                <div className="font-semibold text-purple-900 dark:text-purple-100 mb-1">‚è±Ô∏è Timing:</div>
                <div className="text-purple-700 dark:text-purple-300">
                  Audio Generation: <span className="font-mono font-bold">{(cartesiaAudioTime / 1000).toFixed(2)}s</span>
                </div>
              </div>
            )}
            
            {cartesiaAudio && (
              <div className="mt-4">
                {/* biome-ignore lint/a11y/useMediaCaption: The audio is generated from the text input which serves as the caption */}
                <audio controls className="w-full" key={cartesiaAudio}>
                  <source src={cartesiaAudio} type="audio/mpeg" />
                  Your browser does not support the audio element.
                </audio>
              </div>
            )}
          </div>
        </div>

        <div className="mt-8 bg-yellow-50 dark:bg-yellow-900 border border-yellow-200 dark:border-yellow-700 rounded-lg p-4">
          <p className="text-sm text-yellow-800 dark:text-yellow-200">
            <strong>Note:</strong> Make sure to set up your API keys in the .env
            file. See .env.example for reference.
          </p>
        </div>
      </main>
    </div>
  );
}
